---
description:
globs:
alwaysApply: false
---
# RAG Bootstrap — правки

Ниже — исправления к первоначальным заготовкам: добавлена сборка базового Airflow-образа с системными зависимостями OCR/табличных библиотек, бинд-монтаж каталога входящих документов, устранён конфликт портов API c Airflow, исправлены SQL-схемы (tenant по умолчанию) и PgVectorStore.

---

## 1) `infra/compose/.env.example` — обновление

```env
# ---- Global ----
PROJECT_NAME=rag-platform
TZ=Europe/Dublin

# ---- Postgres ----
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=rag_app
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

PG_RAG_DB=rag_app
PG_AIRFLOW_DB=airflow
PG_SUPERSET_DB=superset

# ---- Redis ----
REDIS_HOST=redis
REDIS_PORT=6379

# ---- Airflow ----
AIRFLOW_FERNET_KEY=PLEASE_GENERATE_32_BYTES_BASE64
AIRFLOW_USER=airflow
AIRFLOW_PASSWORD=airflow
AIRFLOW_EXECUTOR=CeleryExecutor
# Хостовый каталог с документами (монтируется в /opt/airflow/inbox)
INBOX_HOSTDIR=../../data/inbox

# ---- ClickHouse ----
CLICKHOUSE_HOST=clickhouse
CLICKHOUSE_PORT=8123
CLICKHOUSE_DB=rag

# ---- Superset ----
SUPERSET_SECRET_KEY=CHANGE_ME_SUPERSET_SECRET

# ---- Ollama ----
OLLAMA_HOST=ollama
OLLAMA_PORT=11434
OLLAMA_LLM_MODEL=llama3:8b
OLLAMA_EMBED_MODEL=bge-m3

# ---- API (FastAPI) ----
API_HOST=0.0.0.0
API_PORT=8081  # был конфликт с Airflow 8080 → переносим на 8081

# ---- Streamlit ----
STREAMLIT_PORT=8501

# ---- GHCR ----
GHCR_OWNER=your-gh-username-or-org
GHCR_REPO=rag-platform
IMAGE_TAG=latest
```

---

## 2) Новый базовый образ Airflow: `pipelines/airflow/Dockerfile`

```dockerfile
FROM apache/airflow:2.9.3
USER root
# Системные зависимости для OCR/таблиц
RUN apt-get update && apt-get install -y --no-install-recommends \
    tesseract-ocr tesseract-ocr-rus tesseract-ocr-eng \
    poppler-utils ghostscript \
    default-jre-headless \
    libgl1 \
    && rm -rf /var/lib/apt/lists/*
USER airflow
# Python-зависимости будут устанавливаться на этапе airflow-init из requirements.txt
```

---

## 3) `infra/compose/docker-compose.yml` — обновлённые сервисы Airflow и монтирование inbox

```yaml
version: "3.9"
name: ${PROJECT_NAME}

x-env: &default-env
  TZ: ${TZ}

services:
  postgres:
    image: pgvector/pgvector:pg16-bookworm
    container_name: ${PROJECT_NAME}-postgres
    environment:
      <<: *default-env
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./../db/postgres/init:/docker-entrypoint-initdb.d

  redis:
    image: redis:7-alpine
    container_name: ${PROJECT_NAME}-redis
    ports:
      - "${REDIS_PORT}:6379"
    volumes:
      - redisdata:/data

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: ${PROJECT_NAME}-clickhouse
    environment:
      <<: *default-env
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - chdata:/var/lib/clickhouse
      - ./../db/clickhouse/init:/docker-entrypoint-initdb.d

  superset:
    build:
      context: ./../superset
      dockerfile: Dockerfile
    container_name: ${PROJECT_NAME}-superset
    environment:
      <<: *default-env
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_ENV: production
      FLASK_APP: superset
      SUPERSET_DATABASE_URI: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_SUPERSET_DB}
    ports:
      - "8088:8088"
    depends_on: [postgres]
    volumes:
      - superset_home:/app/superset_home

  ollama:
    image: ollama/ollama:latest
    container_name: ${PROJECT_NAME}-ollama
    environment:
      <<: *default-env
      OLLAMA_HOST: 0.0.0.0
    ports:
      - "${OLLAMA_PORT}:11434"
    volumes:
      - ollama:/root/.ollama
    command: ["serve"]

  airflow-init:
    build:
      context: ./../../pipelines/airflow
      dockerfile: Dockerfile
    container_name: ${PROJECT_NAME}-airflow-init
    depends_on: [postgres, redis]
    environment:
      <<: *default-env
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
    volumes:
      - ./../../packages/rag_core:/opt/airflow/rag_core
      - ./../../pipelines/airflow/dags:/opt/airflow/dags
      - ./../../pipelines/airflow/requirements.txt:/requirements.txt
      - ${INBOX_HOSTDIR}:/opt/airflow/inbox
    entrypoint: ["bash", "-c"]
    command: |
      pip install --no-cache-dir -r /requirements.txt && \
      airflow db migrate && \
      airflow users create --username ${AIRFLOW_USER} --password ${AIRFLOW_PASSWORD} --firstname admin --lastname user --role Admin --email admin@example.com || true

  airflow-webserver:
    build: { context: ./../../pipelines/airflow, dockerfile: Dockerfile }
    container_name: ${PROJECT_NAME}-airflow-web
    depends_on: [airflow-init]
    environment:
      <<: *default-env
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      PG_DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_RAG_DB}
    ports: ["8080:8080"]
    command: ["airflow", "webserver"]
    volumes:
      - ./../../packages/rag_core:/opt/airflow/rag_core
      - ./../../pipelines/airflow/dags:/opt/airflow/dags
      - ${INBOX_HOSTDIR}:/opt/airflow/inbox

  airflow-scheduler:
    build: { context: ./../../pipelines/airflow, dockerfile: Dockerfile }
    container_name: ${PROJECT_NAME}-airflow-scheduler
    depends_on: [airflow-init]
    environment:
      <<: *default-env
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      PG_DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_RAG_DB}
    command: ["airflow", "scheduler"]
    volumes:
      - ./../../packages/rag_core:/opt/airflow/rag_core
      - ./../../pipelines/airflow/dags:/opt/airflow/dags
      - ${INBOX_HOSTDIR}:/opt/airflow/inbox

  airflow-worker:
    build: { context: ./../../pipelines/airflow, dockerfile: Dockerfile }
    container_name: ${PROJECT_NAME}-airflow-worker
    depends_on: [airflow-init]
    environment:
      <<: *default-env
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      PG_DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_RAG_DB}
    command: ["airflow", "celery", "worker"]
    volumes:
      - ./../../packages/rag_core:/opt/airflow/rag_core
      - ./../../pipelines/airflow/dags:/opt/airflow/dags
      - ${INBOX_HOSTDIR}:/opt/airflow/inbox

  airflow-triggerer:
    build: { context: ./../../pipelines/airflow, dockerfile: Dockerfile }
    container_name: ${PROJECT_NAME}-airflow-triggerer
    depends_on: [airflow-init]
    environment:
      <<: *default-env
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_AIRFLOW_DB}
      PG_DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_RAG_DB}
    command: ["airflow", "triggerer"]
    volumes:
      - ${INBOX_HOSTDIR}:/opt/airflow/inbox

  api:
    build:
      context: ./../../apps/api
      dockerfile: Dockerfile
    container_name: ${PROJECT_NAME}-api
    environment:
      <<: *default-env
      APP_ENV: production
      OLLAMA_BASE_URL: http://ollama:11434
      PG_DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${PG_RAG_DB}
      REDIS_URL: redis://redis:6379/1
      CLICKHOUSE_URL: http://clickhouse:8123
      CONFIG_PATH: /app/configs/app.toml
    depends_on: [postgres, ollama]
    volumes:
      - ./../../configs:/app/configs
      - ./../../packages/rag_core:/app/rag_core
    ports:
      - "${API_PORT}:8080"

  streamlit:
    build:
      context: ./../../apps/streamlit_app
      dockerfile: Dockerfile
    container_name: ${PROJECT_NAME}-streamlit
    environment:
      <<: *default-env
      API_BASE_URL: http://api:8080
    depends_on: [api]
    ports:
      - "${STREAMLIT_PORT}:8501"

volumes:
  pgdata:
  redisdata:
  chdata:
  superset_home:
  ollama:
```

---

## 4) Postgres init — исправления

### `infra/db/postgres/init/00_users_and_dbs.sql`

```sql
-- Users & DBs
CREATE USER airflow WITH PASSWORD 'airflow';
CREATE USER superset WITH PASSWORD 'superset';

CREATE DATABASE airflow OWNER airflow;
CREATE DATABASE superset OWNER superset;
CREATE DATABASE rag_app OWNER postgres; -- фикс: без переменных окружения
```

### `infra/db/postgres/init/02_rag_app_schema.sql`

```sql
\connect rag_app;

-- tenants/roles & ACL
CREATE TABLE IF NOT EXISTS tenants (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  code TEXT UNIQUE NOT NULL,
  name TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT now()
);

INSERT INTO tenants (code, name)
VALUES ('default', 'Default Tenant')
ON CONFLICT (code) DO NOTHING;

CREATE TABLE IF NOT EXISTS roles (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT UNIQUE NOT NULL
);

CREATE TABLE IF NOT EXISTS users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE,
  email TEXT UNIQUE NOT NULL,
  role_id UUID REFERENCES roles(id),
  created_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE IF NOT EXISTS documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE,
  title TEXT,
  source_path TEXT,
  mime_type TEXT,
  sha256 TEXT UNIQUE,
  size_bytes INT,
  created_at TIMESTAMPTZ DEFAULT now(),
  meta JSONB DEFAULT '{}'::jsonb
);

-- chunk types
DO $$ BEGIN
  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'chunk_kind') THEN
    CREATE TYPE chunk_kind AS ENUM ('text','table');
  END IF;
END $$;

CREATE TABLE IF NOT EXISTS chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  doc_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
  chunk_index INT NOT NULL,
  kind chunk_kind NOT NULL,
  content TEXT,
  table_html TEXT,
  bbox JSONB,
  page_no INT,
  created_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE IF NOT EXISTS embeddings (
  chunk_id UUID PRIMARY KEY REFERENCES chunks(id) ON DELETE CASCADE,
  embedding vector(1024) NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_embeddings_hnsw ON embeddings USING hnsw (embedding vector_cosine_ops);

CREATE TABLE IF NOT EXISTS doc_acl (
  doc_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  role_id UUID REFERENCES roles(id) ON DELETE CASCADE,
  can_read BOOLEAN NOT NULL DEFAULT TRUE,
  PRIMARY KEY (doc_id, role_id)
);
```

---

## 5) `packages/rag_core/rag_core/vectorstore/pgvector_store.py` — исправление выбора tenant

```python
from __future__ import annotations
import json
from typing import List, Dict, Any
import psycopg

class PgVectorStore:
    def __init__(self, dsn: str):
        self.dsn = dsn

    def _default_tenant_id(self, cur) -> str:
        cur.execute("SELECT id FROM tenants WHERE code = 'default' LIMIT 1")
        row = cur.fetchone()
        if row:
            return row[0]
        # fallback (теоретически не должен сработать, т.к. INSERT есть в init)
        cur.execute("INSERT INTO tenants (code, name) VALUES ('default','Default Tenant') RETURNING id")
        return cur.fetchone()[0]

    def ensure_document(self, path: str, sha256: str, title: str) -> str:
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                tenant_id = self._default_tenant_id(cur)
                cur.execute(
                    """
                    INSERT INTO documents (tenant_id, title, source_path, sha256, mime_type, size_bytes)
                    VALUES (%s, %s, %s, %s, NULL, NULL)
                    ON CONFLICT (sha256) DO UPDATE SET title = EXCLUDED.title
                    RETURNING id
                    """,
                    (tenant_id, title, path, sha256),
                )
                return cur.fetchone()[0]

    def upsert_chunks_and_embeddings(self, doc_id: str, payloads: List[Dict[str, Any]], vectors: List[List[float]]):
        assert len(payloads) == len(vectors)
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                for p, v in zip(payloads, vectors):
                    cur.execute(
                        """
                        INSERT INTO chunks (doc_id, chunk_index, kind, content, table_html, bbox, page_no)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                        """,
                        (
                            doc_id,
                            p.get("idx", 0),
                            p.get("kind", "text"),
                            p.get("text"),
                            p.get("table_html"),
                            json.dumps(p.get("bbox")),
                            p.get("page_no"),
                        ),
                    )
                    chunk_id = cur.fetchone()[0]
                    cur.execute(
                        "INSERT INTO embeddings (chunk_id, embedding) VALUES (%s, %s)",
                        (chunk_id, v),
                    )
            conn.commit()

    def search(self, query_vector: List[float], top_k: int = 20) -> List[Dict[str, Any]]:
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT c.id, c.doc_id, c.kind, c.content, c.table_html, 1 - (e.embedding <=> %s) AS score
                    FROM embeddings e
                    JOIN chunks c ON c.id = e.chunk_id
                    ORDER BY e.embedding <=> %s
                    LIMIT %s
                    """,
                    (query_vector, query_vector, top_k),
                )
                cols = [d[0] for d in cur.description]
                return [dict(zip(cols, row)) for row in cur.fetchall()]
```

---

## 6) Пояснение

* Теперь можно класть документы для обработки в каталог `data/inbox` в корне репозитория; контейнеры Airflow видят их в `/opt/airflow/inbox`.
* API слушает `:8081`, Airflow остаётся на `:8080`.
* OCR/табличные утилиты (Tesseract, Poppler, Java, libGL) доступны в контейнерах Airflow.

