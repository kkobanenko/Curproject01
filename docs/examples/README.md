# üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RAG Platform

## üåü –í–≤–µ–¥–µ–Ω–∏–µ

–≠—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RAG Platform –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ –æ—Ç—Ä–∞—Å–ª–µ–π. –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤–∫–ª—é—á–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π –∫–æ–¥, –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

## üìñ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [üè¢ –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∑–Ω–∞–Ω–∏–π](#-–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π-–ø–æ–∏—Å–∫-–∑–Ω–∞–Ω–∏–π)
2. [üìö –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞](#-—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è-–±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
3. [üè• –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](#-–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è)
4. [‚öñÔ∏è –ü—Ä–∞–≤–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã](#-–ø—Ä–∞–≤–æ–≤—ã–µ-–¥–æ–∫—É–º–µ–Ω—Ç—ã)
5. [üî¨ –ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è](#-–Ω–∞—É—á–Ω—ã–µ-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)
6. [üè≠ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](#-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è)
7. [üìä –ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞](#-–±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞)
8. [üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞](#-–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞)

## üè¢ –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∑–Ω–∞–Ω–∏–π

### –°—Ü–µ–Ω–∞—Ä–∏–π
–ö–æ–º–ø–∞–Ω–∏—è —Ö–æ—á–µ—Ç —Å–æ–∑–¥–∞—Ç—å –µ–¥–∏–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, –≤–∫–ª—é—á–∞—é—â—É—é:
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ –ø–æ–ª–∏—Ç–∏–∫–∏
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
- –û—Ç—á–µ—Ç—ã –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫—É
- FAQ –∏ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

#### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
```python
import requests
import os
from pathlib import Path

class CorporateKnowledgeBase:
    def __init__(self, api_base_url, admin_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {admin_token}"}
    
    def setup_document_categories(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        categories = {
            "policies": {
                "name": "–ü–æ–ª–∏—Ç–∏–∫–∏ –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã",
                "tags": ["–ø–æ–ª–∏—Ç–∏–∫–∞", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "—Ä–µ–≥–ª–∞–º–µ–Ω—Ç"],
                "description": "–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏"
            },
            "technical": {
                "name": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", 
                "tags": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"],
                "description": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
            },
            "analytics": {
                "name": "–û—Ç—á–µ—Ç—ã –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞",
                "tags": ["–æ—Ç—á–µ—Ç", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "–º–µ—Ç—Ä–∏–∫–∏"],
                "description": "–ë–∏–∑–Ω–µ—Å-–æ—Ç—á–µ—Ç—ã –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã"
            },
            "support": {
                "name": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏",
                "tags": ["FAQ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "—Ä–µ—à–µ–Ω–∏–µ"],
                "description": "–ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ —Ä–µ—à–µ–Ω–∏—è"
            }
        }
        return categories
    
    def bulk_upload_documents(self, documents_folder):
        """–ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
        results = []
        
        for root, dirs, files in os.walk(documents_folder):
            category = Path(root).name.lower()
            
            for file in files:
                if file.lower().endswith(('.pdf', '.docx', '.txt', '.html')):
                    file_path = os.path.join(root, file)
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏
                    tags = self._determine_tags(file_path, category)
                    
                    try:
                        result = self.upload_document(file_path, tags, category)
                        results.append({
                            "file": file,
                            "status": "success",
                            "document_id": result.get("document", {}).get("id")
                        })
                    except Exception as e:
                        results.append({
                            "file": file,
                            "status": "error", 
                            "error": str(e)
                        })
        
        return results
    
    def upload_document(self, file_path, tags, category):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        with open(file_path, 'rb') as file:
            files = {"file": file}
            data = {
                "tags": tags,
                "metadata": {
                    "category": category,
                    "upload_source": "bulk_import",
                    "department": self._extract_department(file_path)
                }
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/documents",
                headers=self.headers,
                files=files,
                data=data
            )
            response.raise_for_status()
            return response.json()
    
    def _determine_tags(self, file_path, category):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        tags = [category]
        
        path_lower = file_path.lower()
        
        # –¢–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏
        if "hr" in path_lower or "–∫–∞–¥—Ä—ã" in path_lower:
            tags.extend(["HR", "–∫–∞–¥—Ä—ã"])
        elif "it" in path_lower or "–ò–¢" in path_lower:
            tags.extend(["IT", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"])
        elif "finance" in path_lower or "—Ñ–∏–Ω–∞–Ω—Å—ã" in path_lower:
            tags.extend(["—Ñ–∏–Ω–∞–Ω—Å—ã", "–±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è"])
        elif "legal" in path_lower or "–ø—Ä–∞–≤–æ–≤—ã–µ" in path_lower:
            tags.extend(["—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ", "–ø—Ä–∞–≤–æ–≤—ã–µ"])
        
        # –¢–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        if "policy" in path_lower or "–ø–æ–ª–∏—Ç–∏–∫–∞" in path_lower:
            tags.append("–ø–æ–ª–∏—Ç–∏–∫–∞")
        elif "procedure" in path_lower or "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞" in path_lower:
            tags.append("–ø—Ä–æ—Ü–µ–¥—É—Ä–∞")
        elif "manual" in path_lower or "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ" in path_lower:
            tags.append("—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ")
        
        return tags
    
    def _extract_department(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª–∞ –∏–∑ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞"""
        path_parts = Path(file_path).parts
        departments = {
            "hr": "–û—Ç–¥–µ–ª –∫–∞–¥—Ä–æ–≤",
            "it": "IT –æ—Ç–¥–µ–ª", 
            "finance": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª",
            "legal": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –æ—Ç–¥–µ–ª",
            "marketing": "–û—Ç–¥–µ–ª –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞",
            "sales": "–û—Ç–¥–µ–ª –ø—Ä–æ–¥–∞–∂"
        }
        
        for part in path_parts:
            part_lower = part.lower()
            for key, value in departments.items():
                if key in part_lower:
                    return value
        
        return "–û–±—â–∏–π"

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
kb = CorporateKnowledgeBase(
    api_base_url="https://api.rag-platform.com",
    admin_token="YOUR_ADMIN_TOKEN"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
results = kb.bulk_upload_documents("/path/to/corporate/documents/")

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
successful = [r for r in results if r["status"] == "success"]
failed = [r for r in results if r["status"] == "error"]

print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(successful)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
print(f"–û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {len(failed)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
```

#### 2. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ
```python
class CorporateSearch:
    def __init__(self, api_base_url, user_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {user_token}"}
    
    def search_policies(self, query, department=None):
        """–ü–æ–∏—Å–∫ –≤ –ø–æ–ª–∏—Ç–∏–∫–∞—Ö –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö"""
        filters = {
            "tags": ["–ø–æ–ª–∏—Ç–∏–∫–∞", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "—Ä–µ–≥–ª–∞–º–µ–Ω—Ç"]
        }
        
        if department:
            filters["metadata.department"] = department
        
        return self._semantic_search(query, filters)
    
    def search_technical_docs(self, query, technology=None):
        """–ü–æ–∏—Å–∫ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        filters = {
            "tags": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"]
        }
        
        if technology:
            filters["tags"].append(technology)
        
        return self._semantic_search(query, filters)
    
    def search_support_knowledge(self, query):
        """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏"""
        filters = {
            "tags": ["FAQ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "—Ä–µ—à–µ–Ω–∏–µ"]
        }
        
        return self._semantic_search(query, filters)
    
    def get_department_analytics(self, department, date_from=None, date_to=None):
        """–ü–æ–∏—Å–∫ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ –ø–æ –æ—Ç–¥–µ–ª—É"""
        filters = {
            "tags": ["–æ—Ç—á–µ—Ç", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞"],
            "metadata.department": department
        }
        
        if date_from:
            filters["date_from"] = date_from
        if date_to:
            filters["date_to"] = date_to
        
        return self._semantic_search(f"–æ—Ç—á–µ—Ç {department}", filters)
    
    def _semantic_search(self, query, filters):
        """–ë–∞–∑–æ–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
        search_data = {
            "query": query,
            "limit": 20,
            "threshold": 0.7,
            "filters": filters
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/search/semantic",
            headers=self.headers,
            json=search_data
        )
        response.raise_for_status()
        return response.json()
    
    def get_smart_answer(self, question, context_filters=None):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        rag_request = {
            "question": question,
            "context_limit": 8,
            "include_sources": True,
            "filters": context_filters or {},
            "model_params": {
                "temperature": 0.3,  # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                "max_tokens": 800
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/answers/rag",
            headers=self.headers,
            json=rag_request
        )
        response.raise_for_status()
        return response.json()

# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
search = CorporateSearch(
    api_base_url="https://api.rag-platform.com",
    user_token="USER_TOKEN"
)

# –ü–æ–∏—Å–∫ –ø–æ–ª–∏—Ç–∏–∫ –ø–æ –æ—Ç–ø—É—Å–∫–∞–º
vacation_policies = search.search_policies(
    "–ø–æ–ª–∏—Ç–∏–∫–∞ –æ—Ç–ø—É—Å–∫–æ–≤ –∏ –±–æ–ª—å–Ω–∏—á–Ω—ã—Ö",
    department="–û—Ç–¥–µ–ª –∫–∞–¥—Ä–æ–≤"
)

# –ü–æ–∏—Å–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ API
api_docs = search.search_technical_docs(
    "REST API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è",
    technology="API"
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö
answer = search.get_smart_answer(
    "–ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫—É –≤ –¥—Ä—É–≥–æ–π –≥–æ—Ä–æ–¥?",
    context_filters={"tags": ["–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞"]}
)

print("–û—Ç–≤–µ—Ç:", answer["answer"])
print("–ò—Å—Ç–æ—á–Ω–∏–∫–∏:", [s["document_title"] for s in answer["sources"]])
```

#### 3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
```python
class CorporateIntegration:
    def __init__(self, rag_api_url, corp_systems_config):
        self.rag_api = rag_api_url
        self.corp_config = corp_systems_config
    
    def sync_with_sharepoint(self, sharepoint_site, library_name):
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å SharePoint"""
        try:
            from office365.runtime.auth.authentication_context import AuthenticationContext
            from office365.sharepoint.client_context import ClientContext
            
            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ SharePoint
            auth_context = AuthenticationContext(url=sharepoint_site)
            auth_context.acquire_token_for_user(
                username=self.corp_config["sharepoint"]["username"],
                password=self.corp_config["sharepoint"]["password"]
            )
            
            ctx = ClientContext(sharepoint_site, auth_context)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
            library = ctx.web.lists.get_by_title(library_name)
            items = library.items.get().execute_query()
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
            for item in items:
                if item.file_system_object_type == 0:  # –§–∞–π–ª
                    file_info = {
                        "name": item.properties["FileLeafRef"],
                        "url": item.properties["FileRef"],
                        "modified": item.properties["Modified"],
                        "size": item.properties.get("File_x0020_Size", 0)
                    }
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    if self._should_update_file(file_info):
                        self._download_and_upload_file(ctx, file_info)
            
            return {"status": "success", "synced_files": len(items)}
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def sync_with_confluence(self, confluence_base_url, space_key):
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å Confluence"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            auth = (
                self.corp_config["confluence"]["username"],
                self.corp_config["confluence"]["api_token"]
            )
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ Confluence
            url = f"{confluence_base_url}/rest/api/content"
            params = {
                "spaceKey": space_key,
                "expand": "body.storage,version",
                "limit": 100
            }
            
            response = requests.get(url, auth=auth, params=params)
            response.raise_for_status()
            
            pages = response.json()["results"]
            
            for page in pages:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HTML –≤ —Ç–µ–∫—Å—Ç
                soup = BeautifulSoup(page["body"]["storage"]["value"], "html.parser")
                text_content = soup.get_text()
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG Platform
                doc_data = {
                    "title": page["title"],
                    "content": text_content,
                    "source": "confluence",
                    "source_id": page["id"],
                    "tags": ["confluence", space_key],
                    "metadata": {
                        "space": space_key,
                        "version": page["version"]["number"],
                        "confluence_url": f"{confluence_base_url}/pages/viewpage.action?pageId={page['id']}"
                    }
                }
                
                self._create_or_update_document(doc_data)
            
            return {"status": "success", "synced_pages": len(pages)}
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def setup_webhook_notifications(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ webhook —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        webhook_config = {
            "name": "Corporate Systems Integration",
            "url": self.corp_config["webhook_url"],
            "events": [
                "document.uploaded",
                "document.processed", 
                "search.performed",
                "user.registered"
            ],
            "active": True,
            "filters": {
                "tags": ["–≤–∞–∂–Ω–æ", "—Å—Ä–æ—á–Ω–æ"]  # –¢–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
            }
        }
        
        response = requests.post(
            f"{self.rag_api}/api/v1/webhooks/",
            headers={"Authorization": f"Bearer {self.corp_config['admin_token']}"},
            json=webhook_config
        )
        return response.json()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
corp_config = {
    "sharepoint": {
        "username": "service@company.com",
        "password": "service_password"
    },
    "confluence": {
        "username": "service@company.com", 
        "api_token": "confluence_api_token"
    },
    "webhook_url": "https://company.com/rag-webhook",
    "admin_token": "ADMIN_TOKEN"
}

integration = CorporateIntegration(
    rag_api_url="https://api.rag-platform.com",
    corp_systems_config=corp_config
)

# –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å SharePoint
sharepoint_result = integration.sync_with_sharepoint(
    "https://company.sharepoint.com/sites/knowledge",
    "Documents"
)

# –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å Confluence
confluence_result = integration.sync_with_confluence(
    "https://company.atlassian.net/wiki",
    "CORP"
)

print("SharePoint sync:", sharepoint_result)
print("Confluence sync:", confluence_result)
```

## üìö –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞

### –°—Ü–µ–Ω–∞—Ä–∏–π
–°–æ–∑–¥–∞–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–≤–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
- –ö–∞—Ç–∞–ª–æ–≥–∏–∑–∞—Ü–∏—è –∫–Ω–∏–≥ –∏ —Å—Ç–∞—Ç–µ–π
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤
- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

#### 1. –°–∏—Å—Ç–µ–º–∞ –∫–∞—Ç–∞–ª–æ–≥–∏–∑–∞—Ü–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
```python
class DigitalLibrary:
    def __init__(self, api_base_url, librarian_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {librarian_token}"}
    
    def catalog_book(self, book_file_path, metadata):
        """–ö–∞—Ç–∞–ª–æ–≥–∏–∑–∞—Ü–∏—è –∫–Ω–∏–≥–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        biblio_metadata = self._extract_bibliographic_data(metadata)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤
        subject_tags = self._determine_subject_tags(metadata)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–Ω–∏–≥–∏
        with open(book_file_path, 'rb') as file:
            files = {"file": file}
            data = {
                "title": metadata["title"],
                "tags": subject_tags,
                "metadata": biblio_metadata
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/documents",
                headers=self.headers,
                files=files,
                data=data
            )
            return response.json()
    
    def _extract_bibliographic_data(self, metadata):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        return {
            "title": metadata.get("title", ""),
            "author": metadata.get("author", ""),
            "isbn": metadata.get("isbn", ""),
            "publisher": metadata.get("publisher", ""),
            "publication_year": metadata.get("year", ""),
            "pages": metadata.get("pages", 0),
            "language": metadata.get("language", "ru"),
            "subject": metadata.get("subject", []),
            "dewey_decimal": metadata.get("dewey", ""),
            "library_classification": metadata.get("classification", ""),
            "abstract": metadata.get("abstract", ""),
            "keywords": metadata.get("keywords", [])
        }
    
    def _determine_subject_tags(self, metadata):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤"""
        base_tags = ["–±–∏–±–ª–∏–æ—Ç–µ–∫–∞", "–∫–Ω–∏–≥–∞"]
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        subject_mapping = {
            "–∏—Å—Ç–æ—Ä–∏—è": ["–∏—Å—Ç–æ—Ä–∏—è", "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π"],
            "–Ω–∞—É–∫–∞": ["–Ω–∞—É–∫–∞", "–Ω–∞—É—á–Ω—ã–π", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"],
            "—Ç–µ—Ö–Ω–∏–∫–∞": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π", "–∏–Ω–∂–µ–Ω–µ—Ä–∏—è", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è"],
            "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞": ["—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è", "–ø–æ—ç–∑–∏—è", "–ø—Ä–æ–∑–∞"],
            "—Ñ–∏–ª–æ—Å–æ—Ñ–∏—è": ["—Ñ–∏–ª–æ—Å–æ—Ñ–∏—è", "—ç—Ç–∏–∫–∞", "–ª–æ–≥–∏–∫–∞"],
            "–º–µ–¥–∏—Ü–∏–Ω–∞": ["–º–µ–¥–∏—Ü–∏–Ω–∞", "–∑–¥–æ—Ä–æ–≤—å–µ", "–ª–µ—á–µ–Ω–∏–µ"],
            "—ç–∫–æ–Ω–æ–º–∏–∫–∞": ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "–±–∏–∑–Ω–µ—Å"],
            "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ": ["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–ø–µ–¥–∞–≥–æ–≥–∏–∫–∞", "–æ–±—É—á–µ–Ω–∏–µ"]
        }
        
        subject = metadata.get("subject", [])
        keywords = metadata.get("keywords", [])
        
        for category, terms in subject_mapping.items():
            for term in terms:
                if any(term.lower() in s.lower() for s in subject + keywords):
                    base_tags.append(category)
                    break
        
        return base_tags
    
    def batch_catalog_books(self, books_directory):
        """–ú–∞—Å—Å–æ–≤–∞—è –∫–∞—Ç–∞–ª–æ–≥–∏–∑–∞—Ü–∏—è –∫–Ω–∏–≥"""
        results = []
        
        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        for metadata_file in Path(books_directory).glob("*.json"):
            try:
                with open(metadata_file) as f:
                    metadata = json.load(f)
                
                # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –∫–Ω–∏–≥–∏
                book_file = self._find_book_file(metadata_file, metadata)
                
                if book_file and book_file.exists():
                    result = self.catalog_book(str(book_file), metadata)
                    results.append({
                        "book": metadata.get("title", book_file.name),
                        "status": "success",
                        "document_id": result.get("document", {}).get("id")
                    })
                else:
                    results.append({
                        "book": metadata.get("title", metadata_file.stem),
                        "status": "error",
                        "error": "Book file not found"
                    })
                    
            except Exception as e:
                results.append({
                    "book": metadata_file.stem,
                    "status": "error",
                    "error": str(e)
                })
        
        return results

# –ü—Ä–∏–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏
book_metadata = {
    "title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥",
    "author": "–°—Ç—é–∞—Ä—Ç –†–∞—Å—Å–µ–ª, –ü–∏—Ç–µ—Ä –ù–æ—Ä–≤–∏–≥",
    "isbn": "978-5-8459-2006-9",
    "publisher": "–í–∏–ª—å—è–º—Å",
    "year": "2020",
    "pages": 1408,
    "language": "ru",
    "subject": ["–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –Ω–∞—É–∫–∏"],
    "dewey": "006.3",
    "classification": "–£–î–ö 004.8",
    "abstract": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É...",
    "keywords": ["–ò–ò", "–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"]
}

# –ö–∞—Ç–∞–ª–æ–≥–∏–∑–∞—Ü–∏—è
library = DigitalLibrary(
    api_base_url="https://api.rag-platform.com",
    librarian_token="LIBRARIAN_TOKEN"
)

result = library.catalog_book("ai_book.pdf", book_metadata)
```

#### 2. –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
```python
class AcademicResearch:
    def __init__(self, api_base_url, researcher_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {researcher_token}"}
    
    def literature_review(self, research_topic, max_sources=50):
        """–ü–æ–∏—Å–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –¥–ª—è –æ–±–∑–æ—Ä–∞ –ø–æ —Ç–µ–º–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        search_results = self._multi_query_search(research_topic)
        
        # –ê–Ω–∞–ª–∏–∑ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        ranked_sources = self._rank_academic_sources(search_results)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
        bibliography = self._generate_bibliography(ranked_sources[:max_sources])
        
        return {
            "research_topic": research_topic,
            "sources_found": len(ranked_sources),
            "selected_sources": ranked_sources[:max_sources],
            "bibliography": bibliography,
            "research_gaps": self._identify_research_gaps(ranked_sources)
        }
    
    def _multi_query_search(self, research_topic):
        """–ú–Ω–æ–≥–æ–∑–∞–ø—Ä–æ—Å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º—ã"""
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã
        query_variations = [
            research_topic,
            f"{research_topic} –º–µ—Ç–æ–¥—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            f"{research_topic} —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã", 
            f"{research_topic} —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã",
            f"{research_topic} –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"
        ]
        
        all_results = []
        for query in query_variations:
            search_data = {
                "query": query,
                "limit": 30,
                "threshold": 0.6,
                "filters": {
                    "tags": ["–Ω–∞—É–∫–∞", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π"],
                    "metadata.publication_year": {"gte": "2010"}  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10+ –ª–µ—Ç
                }
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/search/semantic",
                headers=self.headers,
                json=search_data
            )
            
            if response.status_code == 200:
                results = response.json()["results"]
                all_results.extend(results)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_results = self._deduplicate_results(all_results)
        return unique_results
    
    def _rank_academic_sources(self, sources):
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏"""
        for source in sources:
            score = 0
            metadata = source.get("metadata", {})
            
            # –§–∞–∫—Ç–æ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            if metadata.get("publication_year"):
                year = int(metadata["publication_year"])
                if year >= 2020:
                    score += 10
                elif year >= 2015:
                    score += 5
            
            # –¢–∏–ø –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            if any(keyword in source["content"].lower() 
                   for keyword in ["–∂—É—Ä–Ω–∞–ª", "conference", "ieee", "acm"]):
                score += 15
            
            # –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            academic_terms = ["–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–∞–Ω–∞–ª–∏–∑", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç", "–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è"]
            term_count = sum(1 for term in academic_terms 
                           if term in source["content"].lower())
            score += term_count * 3
            
            # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (similarity score)
            score += source["score"] * 20
            
            source["academic_score"] = score
        
        return sorted(sources, key=lambda x: x["academic_score"], reverse=True)
    
    def generate_research_summary(self, research_topic, sources):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è RAG
        context_text = "\n\n".join([
            f"–ò—Å—Ç–æ—á–Ω–∏–∫: {s['metadata'].get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')}\n{s['content'][:1000]}"
            for s in sources[:10]
        ])
        
        rag_request = {
            "question": f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–æ–∑–¥–∞–π—Ç–µ 
                          –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ '{research_topic}'.
                          –í–∫–ª—é—á–∏—Ç–µ:
                          1. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
                          2. –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã
                          3. –ü–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è
                          4. –û—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã
                          5. –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è""",
            "context_limit": 15,
            "include_sources": True,
            "model_params": {
                "temperature": 0.4,
                "max_tokens": 1500
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/answers/rag",
            headers=self.headers,
            json=rag_request
        )
        
        return response.json() if response.status_code == 200 else None

# –ü—Ä–∏–º–µ—Ä –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
researcher = AcademicResearch(
    api_base_url="https://api.rag-platform.com",
    researcher_token="RESEARCHER_TOKEN"
)

# –û–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é
literature_review = researcher.literature_review(
    "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"
)

print(f"–ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {literature_review['sources_found']}")
print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(literature_review['selected_sources'])}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
research_summary = researcher.generate_research_summary(
    "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ NLP",
    literature_review['selected_sources']
)

print("–†–µ–∑—é–º–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:", research_summary["answer"])
```

## üè• –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π
–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –¥–ª—è:
- –•—Ä–∞–Ω–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤
- –ü–æ–∏—Å–∫–∞ —Å–∏–º–ø—Ç–æ–º–æ–≤ –∏ –¥–∏–∞–≥–Ω–æ–∑–æ–≤
- –ê–Ω–∞–ª–∏–∑–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ª—É—á–∞–µ–≤
- –°–æ–±–ª—é–¥–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

```python
class MedicalDocumentSystem:
    def __init__(self, api_base_url, medical_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {medical_token}"}
        
        # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.specializations = {
            "–∫–∞—Ä–¥–∏–æ–ª–æ–≥–∏—è": ["—Å–µ—Ä–¥—Ü–µ", "–∫–∞—Ä–¥–∏–æ", "–∞—Ä–∏—Ç–º–∏—è", "–∏–Ω—Ñ–∞—Ä–∫—Ç"],
            "–Ω–µ–≤—Ä–æ–ª–æ–≥–∏—è": ["–Ω–µ—Ä–≤–Ω–∞—è", "–º–æ–∑–≥", "–Ω–µ–≤—Ä–æ", "–∏–Ω—Å—É–ª—å—Ç"],
            "–ø–µ–¥–∏–∞—Ç—Ä–∏—è": ["–¥–µ—Ç–∏", "–¥–µ—Ç—Å–∫–∞—è", "–ø–µ–¥–∏–∞—Ç—Ä"],
            "–æ–Ω–∫–æ–ª–æ–≥–∏—è": ["—Ä–∞–∫", "–æ–ø—É—Ö–æ–ª—å", "–æ–Ω–∫–æ", "–Ω–æ–≤–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"],
            "—Ç–µ—Ä–∞–ø–∏—è": ["—Ç–µ—Ä–∞–ø–µ–≤—Ç", "–æ–±—â–∞—è", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ"],
            "—Ö–∏—Ä—É—Ä–≥–∏—è": ["–æ–ø–µ—Ä–∞—Ü–∏—è", "—Ö–∏—Ä—É—Ä–≥", "–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ"]
        }
    
    def upload_medical_protocol(self, protocol_file, protocol_metadata):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞"""
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        specialization = self._determine_specialization(protocol_metadata)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–æ–¥–æ–≤
        medical_codes = self._extract_medical_codes(protocol_metadata)
        
        tags = [
            "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π", "–ø—Ä–æ—Ç–æ–∫–æ–ª", specialization,
            protocol_metadata.get("protocol_type", "–æ–±—â–∏–π")
        ]
        
        metadata = {
            "specialization": specialization,
            "protocol_type": protocol_metadata.get("protocol_type"),
            "medical_codes": medical_codes,
            "approval_date": protocol_metadata.get("approval_date"),
            "version": protocol_metadata.get("version", "1.0"),
            "authority": protocol_metadata.get("authority", ""),
            "evidence_level": protocol_metadata.get("evidence_level", ""),
            "compliance_required": True
        }
        
        with open(protocol_file, 'rb') as file:
            files = {"file": file}
            data = {
                "title": protocol_metadata["title"],
                "tags": tags,
                "metadata": metadata
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/documents",
                headers=self.headers,
                files=files,
                data=data
            )
            return response.json()
    
    def clinical_decision_support(self, symptoms, patient_data=None):
        """–°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤
        clinical_query = f"—Å–∏–º–ø—Ç–æ–º—ã {symptoms} –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ª–µ—á–µ–Ω–∏–µ"
        
        search_data = {
            "query": clinical_query,
            "limit": 15,
            "threshold": 0.7,
            "filters": {
                "tags": ["–ø—Ä–æ—Ç–æ–∫–æ–ª", "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–ª–µ—á–µ–Ω–∏–µ"],
                "metadata.compliance_required": True
            }
        }
        
        search_response = requests.post(
            f"{self.api_base_url}/api/v1/search/semantic",
            headers=self.headers,
            json=search_data
        )
        
        search_results = search_response.json()["results"]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        clinical_context = ""
        if patient_data:
            clinical_context = f"""
            –î–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞:
            - –í–æ–∑—Ä–∞—Å—Ç: {patient_data.get('age', '–Ω–µ —É–∫–∞–∑–∞–Ω')}
            - –ü–æ–ª: {patient_data.get('gender', '–Ω–µ —É–∫–∞–∑–∞–Ω')}
            - –°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è: {patient_data.get('comorbidities', '–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç')}
            - –ü—Ä–∏–Ω–∏–º–∞–µ–º—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã: {patient_data.get('medications', '–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç')}
            """
        
        rag_request = {
            "question": f"""
            –ù–∞ –æ—Å–Ω–æ–≤–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–∏–º–ø—Ç–æ–º—ã: {symptoms}
            
            {clinical_context}
            
            –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ:
            1. –í–æ–∑–º–æ–∂–Ω—ã–µ –¥–∏–∞–≥–Ω–æ–∑—ã –≤ –ø–æ—Ä—è–¥–∫–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            2. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
            3. –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ä—ã
            4. –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É
            5. –ü—Ä–∏–∑–Ω–∞–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –Ω–µ–æ—Ç–ª–æ–∂–Ω–æ–π –ø–æ–º–æ—â–∏
            
            –í–ê–ñ–ù–û: –≠—Ç–æ —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤—Ä–∞—á.
            """,
            "context_limit": 10,
            "include_sources": True,
            "filters": {
                "tags": ["–ø—Ä–æ—Ç–æ–∫–æ–ª", "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"],
                "metadata.compliance_required": True
            },
            "model_params": {
                "temperature": 0.2,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                "max_tokens": 1000
            }
        }
        
        rag_response = requests.post(
            f"{self.api_base_url}/api/v1/answers/rag",
            headers=self.headers,
            json=rag_request
        )
        
        clinical_recommendations = rag_response.json()
        
        return {
            "symptoms_analyzed": symptoms,
            "relevant_protocols": len(search_results),
            "clinical_recommendations": clinical_recommendations["answer"],
            "evidence_sources": clinical_recommendations["sources"],
            "confidence_level": clinical_recommendations["confidence_score"],
            "disclaimer": "–î–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –¥–∏–∞–≥–Ω–æ–∑ –∏ –ª–µ—á–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–µ—á–∞—â–∏–π –≤—Ä–∞—á."
        }
    
    def drug_interaction_check(self, medications):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"""
        drug_query = f"–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤ {' '.join(medications)}"
        
        search_data = {
            "query": drug_query,
            "limit": 10,
            "filters": {
                "tags": ["—Ñ–∞—Ä–º–∞–∫–æ–ª–æ–≥–∏—è", "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "–ø–æ–±–æ—á–Ω—ã–µ"]
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/search/semantic",
            headers=self.headers,
            json=search_data
        )
        
        return response.json()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã
medical_system = MedicalDocumentSystem(
    api_base_url="https://api.rag-platform.com",
    medical_token="MEDICAL_TOKEN"
)

# –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π —Å–ª—É—á–∞–π
patient_data = {
    "age": 45,
    "gender": "–º—É–∂—Å–∫–æ–π",
    "comorbidities": ["–≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—è", "–¥–∏–∞–±–µ—Ç 2 —Ç–∏–ø–∞"],
    "medications": ["–º–µ—Ç—Ñ–æ—Ä–º–∏–Ω", "—ç–Ω–∞–ª–∞–ø—Ä–∏–ª"]
}

symptoms = "–±–æ–ª—å –≤ –≥—Ä—É–¥–∏, –æ–¥—ã—à–∫–∞, –ø–æ–≤—ã—à–µ–Ω–Ω–æ–µ –ø–æ—Ç–æ–æ—Ç–¥–µ–ª–µ–Ω–∏–µ"

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
clinical_support = medical_system.clinical_decision_support(
    symptoms=symptoms,
    patient_data=patient_data
)

print("–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
print(clinical_support["clinical_recommendations"])
print(f"–£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: {clinical_support['confidence_level']:.1%}")
```

## ‚öñÔ∏è –ü—Ä–∞–≤–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

### –°—Ü–µ–Ω–∞—Ä–∏–π
–°–∏—Å—Ç–µ–º–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ñ–∏—Ä–º –∏ –ø—Ä–∞–≤–æ–≤—ã—Ö –æ—Ç–¥–µ–ª–æ–≤:
- –ë–∞–∑–∞ –ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–æ–≤ –∏ —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏
- –ü–æ–∏—Å–∫ –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–æ–≤ –∏ –∞–Ω–∞–ª–æ–≥–∏–π
- –ê–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏–π
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

```python
class LegalDocumentSystem:
    def __init__(self, api_base_url, legal_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {legal_token}"}
        
        # –¢–∏–ø—ã –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.document_types = {
            "–∑–∞–∫–æ–Ω": ["—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω", "–∫–æ–¥–µ–∫—Å", "–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è"],
            "–ø–æ–¥–∑–∞–∫–æ–Ω–Ω—ã–π_–∞–∫—Ç": ["–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ", "–ø—Ä–∏–∫–∞–∑", "–ø–æ–ª–æ–∂–µ–Ω–∏–µ"],
            "—Å—É–¥–µ–±–Ω–∞—è_–ø—Ä–∞–∫—Ç–∏–∫–∞": ["—Ä–µ—à–µ–Ω–∏–µ —Å—É–¥–∞", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É–¥–∞"],
            "–¥–æ–≥–æ–≤–æ—Ä": ["—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "–¥–æ–≥–æ–≤–æ—Ä"],
            "–ø—Ä–∞–≤–æ–≤–∞—è_–ø–æ–∑–∏—Ü–∏—è": ["—Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏–µ", "–ø–∏—Å—å–º–æ", "–ø–æ–∑–∏—Ü–∏—è"]
        }
    
    def upload_legal_document(self, doc_file, legal_metadata):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π"""
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_type = self._classify_legal_document(legal_metadata)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Ä–µ–∫–≤–∏–∑–∏—Ç–æ–≤
        legal_attributes = self._extract_legal_attributes(legal_metadata)
        
        tags = [
            "–ø—Ä–∞–≤–æ–≤–æ–π", doc_type,
            legal_metadata.get("legal_area", "–æ–±—â–µ–µ –ø—Ä–∞–≤–æ"),
            legal_metadata.get("jurisdiction", "–†–§")
        ]
        
        metadata = {
            "document_type": doc_type,
            "legal_area": legal_metadata.get("legal_area"),
            "jurisdiction": legal_metadata.get("jurisdiction", "–†–§"),
            "legal_force": legal_metadata.get("legal_force", "–¥–µ–π—Å—Ç–≤—É—é—â–∏–π"),
            "adoption_date": legal_metadata.get("adoption_date"),
            "effective_date": legal_metadata.get("effective_date"),
            "document_number": legal_metadata.get("document_number"),
            "issuing_authority": legal_metadata.get("issuing_authority"),
            "legal_hierarchy_level": legal_metadata.get("hierarchy_level"),
            "related_documents": legal_metadata.get("related_documents", []),
            "keywords": legal_metadata.get("keywords", [])
        }
        
        with open(doc_file, 'rb') as file:
            files = {"file": file}
            data = {
                "title": legal_metadata["title"],
                "tags": tags,
                "metadata": metadata
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/documents",
                headers=self.headers,
                files=files,
                data=data
            )
            return response.json()
    
    def legal_research(self, legal_question, research_context=None):
        """–ü—Ä–∞–≤–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É"""
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∞–≤–∞
        legal_area = self._determine_legal_area(legal_question)
        
        # –ú–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –ø–æ–∏—Å–∫
        search_queries = [
            legal_question,
            f"{legal_question} —Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞",
            f"{legal_question} –ø—Ä–∞–≤–æ–≤–æ–µ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ",
            f"{legal_question} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∑–∞–∫–æ–Ω–∞"
        ]
        
        all_results = []
        for query in search_queries:
            search_data = {
                "query": query,
                "limit": 20,
                "threshold": 0.6,
                "filters": {
                    "tags": ["–ø—Ä–∞–≤–æ–≤–æ–π"],
                    "metadata.legal_area": legal_area,
                    "metadata.legal_force": "–¥–µ–π—Å—Ç–≤—É—é—â–∏–π"
                }
            }
            
            if research_context and "jurisdiction" in research_context:
                search_data["filters"]["metadata.jurisdiction"] = research_context["jurisdiction"]
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/search/semantic",
                headers=self.headers,
                json=search_data
            )
            
            if response.status_code == 200:
                results = response.json()["results"]
                all_results.extend(results)
        
        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –ø—Ä–∞–≤–æ–≤–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        ranked_results = self._rank_legal_sources(all_results)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–æ–≤–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è
        legal_opinion = self._generate_legal_opinion(legal_question, ranked_results[:10])
        
        return {
            "legal_question": legal_question,
            "legal_area": legal_area,
            "sources_found": len(ranked_results),
            "primary_sources": [r for r in ranked_results if r.get("is_primary_source")],
            "judicial_practice": [r for r in ranked_results if "—Å—É–¥–µ–±–Ω–∞—è_–ø—Ä–∞–∫—Ç–∏–∫–∞" in r.get("metadata", {}).get("document_type", "")],
            "legal_opinion": legal_opinion,
            "research_confidence": legal_opinion.get("confidence_score", 0)
        }
    
    def contract_analysis(self, contract_text, analysis_focus=None):
        """–ê–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ä–∏—Å–∫–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è"""
        rag_request = {
            "question": f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è:
            
            1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–µ–π—Å—Ç–≤—É—é—â–µ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –†–§
            2. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤ –¥–ª—è —Å—Ç–æ—Ä–æ–Ω
            3. –ù–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
            4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
            5. –°—Å—ã–ª–æ–∫ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞
            
            {f'–û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª–∏—Ç–µ: {analysis_focus}' if analysis_focus else ''}
            
            –¢–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
            {contract_text[:2000]}...
            """,
            "context_limit": 15,
            "include_sources": True,
            "filters": {
                "tags": ["–¥–æ–≥–æ–≤–æ—Ä", "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ –ø—Ä–∞–≤–æ", "–ø—Ä–∞–≤–æ–≤–æ–π"],
                "metadata.legal_force": "–¥–µ–π—Å—Ç–≤—É—é—â–∏–π"
            },
            "model_params": {
                "temperature": 0.3,
                "max_tokens": 1200
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/answers/rag",
            headers=self.headers,
            json=rag_request
        )
        
        analysis_result = response.json()
        
        return {
            "contract_analysis": analysis_result["answer"],
            "legal_sources": analysis_result["sources"],
            "risk_level": self._assess_contract_risk(analysis_result),
            "recommendations": self._extract_recommendations(analysis_result["answer"])
        }
    
    def _rank_legal_sources(self, sources):
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Å–∏–ª–µ"""
        hierarchy_weights = {
            "–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è": 100,
            "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω": 90,
            "–∫–æ–¥–µ–∫—Å": 85,
            "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞": 70,
            "–ø—Ä–∏–∫–∞–∑ –º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞": 60,
            "—Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –í–°": 80,
            "—Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –ö–°": 95,
            "—Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –æ–±—â–µ–π —é—Ä–∏—Å–¥–∏–∫—Ü–∏–∏": 50
        }
        
        for source in sources:
            base_score = source["score"] * 10
            metadata = source.get("metadata", {})
            
            # –í–µ—Å –ø–æ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø—Ä–∞–≤–∞
            doc_type = metadata.get("document_type", "").lower()
            hierarchy_weight = 0
            for doc_pattern, weight in hierarchy_weights.items():
                if doc_pattern in doc_type:
                    hierarchy_weight = weight
                    break
            
            # –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if metadata.get("legal_force") == "–¥–µ–π—Å—Ç–≤—É—é—â–∏–π":
                actuality_weight = 20
            elif metadata.get("legal_force") == "—É—Ç—Ä–∞—Ç–∏–ª —Å–∏–ª—É":
                actuality_weight = -50
            else:
                actuality_weight = 0
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–π—Ç–∏–Ω–≥
            source["legal_score"] = base_score + hierarchy_weight + actuality_weight
            source["is_primary_source"] = hierarchy_weight >= 70
        
        return sorted(sources, key=lambda x: x["legal_score"], reverse=True)

# –ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
legal_system = LegalDocumentSystem(
    api_base_url="https://api.rag-platform.com",
    legal_token="LEGAL_TOKEN"
)

# –ü—Ä–∞–≤–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
legal_question = "–ü—Ä–∞–≤–∞ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞ –ø—Ä–∏ —É–≤–æ–ª—å–Ω–µ–Ω–∏–∏ –ø–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—é —à—Ç–∞—Ç–æ–≤"
research_context = {"jurisdiction": "–†–§"}

legal_research = legal_system.legal_research(legal_question, research_context)

print(f"–û–±–ª–∞—Å—Ç—å –ø—Ä–∞–≤–∞: {legal_research['legal_area']}")
print(f"–ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {legal_research['sources_found']}")
print(f"–ü–µ—Ä–≤–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(legal_research['primary_sources'])}")
print(f"–°—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏: {len(legal_research['judicial_practice'])}")
print("\n–ü—Ä–∞–≤–æ–≤–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ:")
print(legal_research['legal_opinion']['answer'])
```

–ü—Ä–æ–¥–æ–ª–∂—É —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è...

## üî¨ –ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π
–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –Ω–∞—É—á–Ω–æ-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω—Å—Ç–∏—Ç—É—Ç–æ–≤:
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã–º–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏
- –ê–Ω–∞–ª–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü–æ–∏—Å–∫ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–π –∏ –≥—Ä–∞–Ω—Ç–æ–≤
- –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

```python
class ScientificResearchPlatform:
    def __init__(self, api_base_url, research_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {research_token}"}
        
        # –ù–∞—É—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏
        self.research_fields = {
            "—Ñ–∏–∑–∏–∫–∞": ["–∫–≤–∞–Ω—Ç–æ–≤–∞—è", "—è–¥–µ—Ä–Ω–∞—è", "—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è"],
            "—Ö–∏–º–∏—è": ["–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è", "–Ω–µ–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è", "—Ñ–∏–∑–∏—á–µ—Å–∫–∞—è", "–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è"],
            "–±–∏–æ–ª–æ–≥–∏—è": ["–º–æ–ª–µ–∫—É–ª—è—Ä–Ω–∞—è", "–∫–ª–µ—Ç–æ—á–Ω–∞—è", "–≥–µ–Ω–µ—Ç–∏–∫–∞", "—ç–∫–æ–ª–æ–≥–∏—è"],
            "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞": ["–∞–ª–≥–µ–±—Ä–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è", "–∞–Ω–∞–ª–∏–∑", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"],
            "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞": ["–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö", "—Å–µ—Ç–∏"],
            "–º–µ–¥–∏—Ü–∏–Ω–∞": ["–∫–ª–∏–Ω–∏—á–µ—Å–∫–∞—è", "—Ñ–∞—Ä–º–∞–∫–æ–ª–æ–≥–∏—è", "—ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—è", "–±–∏–æ–º–µ–¥–∏—Ü–∏–Ω–∞"]
        }
    
    def submit_research_paper(self, paper_file, paper_metadata):
        """–ü–æ–¥–∞—á–∞ –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π"""
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –Ω–∞—É—á–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
        research_field = self._classify_research_field(paper_metadata)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        scientific_metrics = self._extract_scientific_metrics(paper_metadata)
        
        tags = [
            "–Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è", research_field,
            paper_metadata.get("publication_type", "–∂—É—Ä–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è"),
            paper_metadata.get("research_type", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ")
        ]
        
        metadata = {
            "research_field": research_field,
            "authors": paper_metadata.get("authors", []),
            "affiliations": paper_metadata.get("affiliations", []),
            "publication_date": paper_metadata.get("publication_date"),
            "journal": paper_metadata.get("journal"),
            "doi": paper_metadata.get("doi"),
            "impact_factor": paper_metadata.get("impact_factor"),
            "keywords": paper_metadata.get("keywords", []),
            "abstract": paper_metadata.get("abstract"),
            "research_methods": paper_metadata.get("methods", []),
            "funding_sources": paper_metadata.get("funding", []),
            "ethical_approval": paper_metadata.get("ethical_approval"),
            "data_availability": paper_metadata.get("data_availability"),
            "supplementary_materials": paper_metadata.get("supplementary", [])
        }
        
        with open(paper_file, 'rb') as file:
            files = {"file": file}
            data = {
                "title": paper_metadata["title"],
                "tags": tags,
                "metadata": metadata
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/documents",
                headers=self.headers,
                files=files,
                data=data
            )
            return response.json()
    
    def research_collaboration_finder(self, research_interests, author_profile=None):
        """–ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–π"""
        # –ü–æ–∏—Å–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏
        search_queries = [
            f"–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è {interest}" for interest in research_interests
        ]
        
        collaboration_candidates = []
        
        for query in search_queries:
            search_data = {
                "query": query,
                "limit": 15,
                "threshold": 0.7,
                "filters": {
                    "tags": ["–Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è"],
                    "metadata.publication_date": {"gte": "2020-01-01"}  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 4 –≥–æ–¥–∞
                }
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/search/semantic",
                headers=self.headers,
                json=search_data
            )
            
            if response.status_code == 200:
                results = response.json()["results"]
                for result in results:
                    authors = result.get("metadata", {}).get("authors", [])
                    affiliations = result.get("metadata", {}).get("affiliations", [])
                    
                    for author in authors:
                        if author_profile and author.lower() != author_profile.get("name", "").lower():
                            collaboration_candidates.append({
                                "author": author,
                                "affiliation": affiliations[0] if affiliations else "Unknown",
                                "research_area": result.get("metadata", {}).get("research_field"),
                                "recent_work": result.get("metadata", {}).get("title"),
                                "relevance_score": result["score"]
                            })
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        author_scores = {}
        for candidate in collaboration_candidates:
            author_key = f"{candidate['author']}_{candidate['affiliation']}"
            if author_key not in author_scores:
                author_scores[author_key] = {
                    "author": candidate["author"],
                    "affiliation": candidate["affiliation"],
                    "research_areas": set([candidate["research_area"]]),
                    "publications_count": 0,
                    "avg_relevance": 0,
                    "total_relevance": 0
                }
            
            author_scores[author_key]["publications_count"] += 1
            author_scores[author_key]["total_relevance"] += candidate["relevance_score"]
            author_scores[author_key]["research_areas"].add(candidate["research_area"])
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞
        for author_data in author_scores.values():
            author_data["avg_relevance"] = author_data["total_relevance"] / author_data["publications_count"]
            author_data["research_areas"] = list(author_data["research_areas"])
            author_data["collaboration_score"] = (
                author_data["avg_relevance"] * 10 +
                author_data["publications_count"] * 2 +
                len(author_data["research_areas"]) * 5
            )
        
        return sorted(author_scores.values(), 
                     key=lambda x: x["collaboration_score"], 
                     reverse=True)[:20]
    
    def grant_opportunity_analysis(self, research_proposal):
        """–ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥—Ä–∞–Ω—Ç–æ–≤–æ–≥–æ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è"""
        rag_request = {
            "question": f"""
            –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞–π–¥–∏—Ç–µ –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ:
            
            1. –ü–æ–¥—Ö–æ–¥—è—â–∏–µ –≥—Ä–∞–Ω—Ç–æ–≤—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ —Ñ–æ–Ω–¥—ã
            2. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∑–∞—è–≤–∫–∞–º –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏
            3. –†–∞–∑–º–µ—Ä—ã —Ç–∏–ø–∏—á–Ω–æ–≥–æ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è
            4. –£—Å–ø–µ—à–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
            5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∑–∞—è–≤–∫–∏
            
            –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞:
            {research_proposal}
            """,
            "context_limit": 12,
            "include_sources": True,
            "filters": {
                "tags": ["–≥—Ä–∞–Ω—Ç", "—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ", "–Ω–∞—É—á–Ω—ã–π —Ñ–æ–Ω–¥"],
            },
            "model_params": {
                "temperature": 0.4,
                "max_tokens": 1000
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/answers/rag",
            headers=self.headers,
            json=rag_request
        )
        
        return response.json() if response.status_code == 200 else None
    
    def citation_impact_analysis(self, author_name, timeframe_years=5):
        """–ê–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –Ω–∞—É—á–Ω–æ–≥–æ –∏–º–ø–∞–∫—Ç–∞"""
        # –ü–æ–∏—Å–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∞–≤—Ç–æ—Ä–∞
        search_data = {
            "query": f"–∞–≤—Ç–æ—Ä {author_name}",
            "limit": 50,
            "filters": {
                "tags": ["–Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è"],
                "metadata.authors": author_name,
                "metadata.publication_date": {
                    "gte": f"{datetime.now().year - timeframe_years}-01-01"
                }
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/search/semantic",
            headers=self.headers,
            json=search_data
        )
        
        if response.status_code != 200:
            return None
        
        publications = response.json()["results"]
        
        # –ê–Ω–∞–ª–∏–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        impact_metrics = {
            "total_publications": len(publications),
            "research_fields": {},
            "collaboration_networks": set(),
            "journal_impact_factors": [],
            "publication_trends": {},
            "h_index_estimate": 0
        }
        
        for pub in publications:
            metadata = pub.get("metadata", {})
            
            # –û–±–ª–∞—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
            field = metadata.get("research_field", "Unknown")
            impact_metrics["research_fields"][field] = impact_metrics["research_fields"].get(field, 0) + 1
            
            # –°–µ—Ç–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞
            authors = metadata.get("authors", [])
            for author in authors:
                if author.lower() != author_name.lower():
                    impact_metrics["collaboration_networks"].add(author)
            
            # –ò–º–ø–∞–∫—Ç-—Ñ–∞–∫—Ç–æ—Ä—ã –∂—É—Ä–Ω–∞–ª–æ–≤
            if metadata.get("impact_factor"):
                impact_metrics["journal_impact_factors"].append(float(metadata["impact_factor"]))
            
            # –¢—Ä–µ–Ω–¥—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–π –ø–æ –≥–æ–¥–∞–º
            pub_date = metadata.get("publication_date", "")
            if pub_date:
                year = pub_date[:4]
                impact_metrics["publication_trends"][year] = impact_metrics["publication_trends"].get(year, 0) + 1
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        impact_metrics["collaboration_networks"] = list(impact_metrics["collaboration_networks"])
        impact_metrics["avg_impact_factor"] = (
            sum(impact_metrics["journal_impact_factors"]) / len(impact_metrics["journal_impact_factors"])
            if impact_metrics["journal_impact_factors"] else 0
        )
        
        return impact_metrics

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞—É—á–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
research_platform = ScientificResearchPlatform(
    api_base_url="https://api.rag-platform.com",
    research_token="RESEARCH_TOKEN"
)

# –ü–æ–∏—Å–∫ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–π
research_interests = ["–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"]
author_profile = {"name": "–ò.–ò. –ò–≤–∞–Ω–æ–≤", "affiliation": "–ú–ì–£"}

collaborations = research_platform.research_collaboration_finder(
    research_interests=research_interests,
    author_profile=author_profile
)

print("–¢–æ–ø-5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏:")
for i, candidate in enumerate(collaborations[:5], 1):
    print(f"{i}. {candidate['author']} ({candidate['affiliation']})")
    print(f"   –û–±–ª–∞—Å—Ç–∏: {', '.join(candidate['research_areas'])}")
    print(f"   –†–µ–π—Ç–∏–Ω–≥: {candidate['collaboration_score']:.1f}")
    print()
```

–°–æ–∑–¥–∞—é –æ—Å—Ç–∞–ª—å–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é:

## üìä –ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞

### –°—Ü–µ–Ω–∞—Ä–∏–π
–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–¥–µ–ª–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π:
- –ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–æ—Ç—á–µ—Ç–æ–≤ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
- –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

```python
class BusinessAnalyticsPlatform:
    def __init__(self, api_base_url, analyst_token):
        self.api_base_url = api_base_url
        self.headers = {"Authorization": f"Bearer {analyst_token}"}
    
    def market_trend_analysis(self, industry, time_period="2023-2024"):
        """–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ –æ—Ç—Ä–∞—Å–ª–∏"""
        trend_queries = [
            f"{industry} —Ä—ã–Ω–æ—á–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ {time_period}",
            f"{industry} –ø—Ä–æ–≥–Ω–æ–∑ —Ä–∞–∑–≤–∏—Ç–∏—è",
            f"{industry} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –∞–Ω–∞–ª–∏–∑",
            f"{industry} –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ"
        ]
        
        trend_data = []
        for query in trend_queries:
            search_data = {
                "query": query,
                "limit": 15,
                "threshold": 0.6,
                "filters": {
                    "tags": ["–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "–æ—Ç—á–µ—Ç", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"],
                    "metadata.report_type": "market_analysis"
                }
            }
            
            response = requests.post(
                f"{self.api_base_url}/api/v1/search/semantic",
                headers=self.headers,
                json=search_data
            )
            
            if response.status_code == 200:
                trend_data.extend(response.json()["results"])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        rag_request = {
            "question": f"""
            –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –æ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è—Ö –≤ –æ—Ç—Ä–∞—Å–ª–∏ {industry}:
            
            1. –ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏ –¥—Ä–∞–π–≤–µ—Ä—ã —Ä–æ—Å—Ç–∞
            2. –†–∞–∑–º–µ—Ä —Ä—ã–Ω–∫–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã
            3. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è —Å—Ä–µ–¥–∞
            4. –ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
            5. –†–∏—Å–∫–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
            6. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞
            """,
            "context_limit": 20,
            "include_sources": True,
            "model_params": {
                "temperature": 0.4,
                "max_tokens": 1500
            }
        }
        
        response = requests.post(
            f"{self.api_base_url}/api/v1/answers/rag",
            headers=self.headers,
            json=rag_request
        )
        
        return response.json() if response.status_code == 200 else None

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
analytics = BusinessAnalyticsPlatform(
    api_base_url="https://api.rag-platform.com",
    analyst_token="ANALYST_TOKEN"
)

market_report = analytics.market_trend_analysis("—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è")
print("–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞:", market_report["answer"])
```

–°–æ–∑–¥–∞—é —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –≤–∏–¥–µ–æ-—Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤:

<function_calls>
<invoke name="todo_write">
<parameter name="merge">true
